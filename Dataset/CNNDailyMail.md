# CNN/DailyMail

**출처: CONLL 2016** [Abstractive Text Summarization using Sequence-to-sequence RNNs and
Beyond](https://arxiv.org/pdf/1602.06023v5.pdf)  
  
[**Homepage**](https://github.com/abisee/cnn-dailymail)

- 텍스트 요약을 위한 데이터 셋.  
- Abstractive 요약은 인간이 생성. 글머리 기호로표시.  
- Story는 빈칸 채우기 문제의 답변으로 예상되는 내용.
- 286,817개의 훈련 쌍, 13,368개의 검증 쌍 및 11,487개의 테스트 쌍.  
- 훈련 세트의 소스 문서는 평균 29.74개의 문장에 걸쳐 766개의 단어를 가지고 있으며 요약은 53개의 단어와 3.72개의 문장으로 구성.

### Benchmark
1. [추상적인 텍스트 요약](https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily)
2. [문서 요약](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail)
3. [질문 답변](https://paperswithcode.com/sota/question-answering-on-cnn-daily-mail)
4. [텍스트 요약](https://paperswithcode.com/sota/text-summarization-on-cnn-daily-mail-2)
5. [추출 텍스트 요약](https://paperswithcode.com/sota/extractive-document-summarization-on-cnn)

### Papers
1. [Attention Is All You Need]
2. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre)
3. [Stepwise Extractive Summarization and Planning with Structured Transformers](https://paperswithcode.com/paper/stepwise-extractive-summarization-and)

---
Extractive Text Summarization 분야에서 많이 사용되는 데이터 셋  
Extractive 외 다른 Text Summarization 분야에서도 많이 사용되기 때문에 비교 분석이 가능할 수 있다는 점에서 좋다고 판단
